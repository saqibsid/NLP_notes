{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom string import punctuation\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom keras.preprocessing import text, sequence\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\nimport tensorflow as tf\n\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:22.930910Z","iopub.execute_input":"2023-09-20T14:51:22.931669Z","iopub.status.idle":"2023-09-20T14:51:27.951243Z","shell.execute_reply.started":"2023-09-20T14:51:22.931596Z","shell.execute_reply":"2023-09-20T14:51:27.949981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:27.953665Z","iopub.execute_input":"2023-09-20T14:51:27.954781Z","iopub.status.idle":"2023-09-20T14:51:28.120784Z","shell.execute_reply.started":"2023-09-20T14:51:27.954739Z","shell.execute_reply":"2023-09-20T14:51:28.119966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping link of article","metadata":{}},{"cell_type":"code","source":"df = df.drop('article_link',axis = 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.122108Z","iopub.execute_input":"2023-09-20T14:51:28.122859Z","iopub.status.idle":"2023-09-20T14:51:28.137605Z","shell.execute_reply.started":"2023-09-20T14:51:28.122816Z","shell.execute_reply":"2023-09-20T14:51:28.136158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking for nulls**","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.139725Z","iopub.execute_input":"2023-09-20T14:51:28.140160Z","iopub.status.idle":"2023-09-20T14:51:28.159141Z","shell.execute_reply.started":"2023-09-20T14:51:28.140124Z","shell.execute_reply":"2023-09-20T14:51:28.157651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot to check class imbalance**","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df, x=\"is_sarcastic\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.163604Z","iopub.execute_input":"2023-09-20T14:51:28.164374Z","iopub.status.idle":"2023-09-20T14:51:28.398653Z","shell.execute_reply.started":"2023-09-20T14:51:28.164330Z","shell.execute_reply":"2023-09-20T14:51:28.397345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Step by step -\n\n1. `stopwords.words('english')`: This line of code imports a list of English stopwords from the NLTK (Natural Language Toolkit) library. Stopwords are common words like \"the,\" \"and,\" \"is,\" \"in,\" etc., that are often removed from text data because they don't carry significant meaning in many NLP tasks.\n\n2. `set(stopwords.words('english'))`: The stopwords are loaded into a Python set data structure. Using a set is efficient for checking whether a word is a stopword or not because it allows for fast membership tests.\n\n3. `string.punctuation`: This line imports a string containing all the common punctuation marks in English, such as \".\", \",\", \"!\", \"?\", etc.\n\n3. `list(string.punctuation)`: The punctuation marks are converted into a list. \n\n4. `stop.update(punctuation)`: Finally, the punctuation marks are added to the set of stopwords. This step ensures that both stopwords and punctuation marks are combined into a single set for later use in text preprocessing.\n\nAfter executing this code, the stop set will contain both English stopwords and punctuation marks.","metadata":{}},{"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.400369Z","iopub.execute_input":"2023-09-20T14:51:28.400714Z","iopub.status.idle":"2023-09-20T14:51:28.406960Z","shell.execute_reply.started":"2023-09-20T14:51:28.400684Z","shell.execute_reply":"2023-09-20T14:51:28.406112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to take input containing HTML and uses BS to remove HTML tags and return plain text.","metadata":{}},{"cell_type":"code","source":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.408444Z","iopub.execute_input":"2023-09-20T14:51:28.409084Z","iopub.status.idle":"2023-09-20T14:51:28.421266Z","shell.execute_reply.started":"2023-09-20T14:51:28.409033Z","shell.execute_reply":"2023-09-20T14:51:28.419789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removal of the square brackets","metadata":{}},{"cell_type":"code","source":"def remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.423196Z","iopub.execute_input":"2023-09-20T14:51:28.423543Z","iopub.status.idle":"2023-09-20T14:51:28.436408Z","shell.execute_reply.started":"2023-09-20T14:51:28.423514Z","shell.execute_reply":"2023-09-20T14:51:28.434976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removal of the URLs","metadata":{}},{"cell_type":"code","source":"def remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.437628Z","iopub.execute_input":"2023-09-20T14:51:28.438000Z","iopub.status.idle":"2023-09-20T14:51:28.450660Z","shell.execute_reply.started":"2023-09-20T14:51:28.437966Z","shell.execute_reply":"2023-09-20T14:51:28.449601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removal of the stopwords from text","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.452003Z","iopub.execute_input":"2023-09-20T14:51:28.452544Z","iopub.status.idle":"2023-09-20T14:51:28.465533Z","shell.execute_reply.started":"2023-09-20T14:51:28.452511Z","shell.execute_reply":"2023-09-20T14:51:28.464109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the data","metadata":{}},{"cell_type":"code","source":"def cleanse_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.467168Z","iopub.execute_input":"2023-09-20T14:51:28.468167Z","iopub.status.idle":"2023-09-20T14:51:28.480864Z","shell.execute_reply.started":"2023-09-20T14:51:28.468122Z","shell.execute_reply":"2023-09-20T14:51:28.479345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Apply function on review column**","metadata":{}},{"cell_type":"code","source":"%time\ndf['headline']=df['headline'].apply(cleanse_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:28.482373Z","iopub.execute_input":"2023-09-20T14:51:28.482965Z","iopub.status.idle":"2023-09-20T14:51:30.153824Z","shell.execute_reply.started":"2023-09-20T14:51:28.482920Z","shell.execute_reply":"2023-09-20T14:51:30.152578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**WordCloud for Text that is Not Sarcastic**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12,10)) \nplt.title(\"WordCloud for Not Sarcastic\")\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:30.155210Z","iopub.execute_input":"2023-09-20T14:51:30.155596Z","iopub.status.idle":"2023-09-20T14:51:50.064573Z","shell.execute_reply.started":"2023-09-20T14:51:30.155565Z","shell.execute_reply":"2023-09-20T14:51:50.063406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WordCloud for Text that is Sarcastic**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12,10)) \nplt.title(\"WordCloud for Sarcastic\")\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:51:50.069984Z","iopub.execute_input":"2023-09-20T14:51:50.071048Z","iopub.status.idle":"2023-09-20T14:52:09.113422Z","shell.execute_reply.started":"2023-09-20T14:51:50.070998Z","shell.execute_reply":"2023-09-20T14:52:09.112308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparing ccharacters in Text classes**","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=df[df['is_sarcastic']==1]['headline'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Sarcastic text')\ntext_len=df[df['is_sarcastic']==0]['headline'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Characters in text classes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:09.114933Z","iopub.execute_input":"2023-09-20T14:52:09.115299Z","iopub.status.idle":"2023-09-20T14:52:09.547801Z","shell.execute_reply.started":"2023-09-20T14:52:09.115267Z","shell.execute_reply":"2023-09-20T14:52:09.546622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparing Words in Text classes**","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=df[df['is_sarcastic']==1]['headline'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Sarcastic text')\ntext_len=df[df['is_sarcastic']==0]['headline'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Words in text classes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:09.549398Z","iopub.execute_input":"2023-09-20T14:52:09.549872Z","iopub.status.idle":"2023-09-20T14:52:10.033922Z","shell.execute_reply.started":"2023-09-20T14:52:09.549829Z","shell.execute_reply":"2023-09-20T14:52:10.032661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparing Avg Word Length in Each Text Class**","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=df[df['is_sarcastic']==1]['headline'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Sarcastic text')\nword=df[df['is_sarcastic']==0]['headline'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Average word length in each text class')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:10.035338Z","iopub.execute_input":"2023-09-20T14:52:10.035686Z","iopub.status.idle":"2023-09-20T14:52:11.687626Z","shell.execute_reply.started":"2023-09-20T14:52:10.035655Z","shell.execute_reply":"2023-09-20T14:52:11.686225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Introduction to Word2Vec","metadata":{}},{"cell_type":"markdown","source":"Reformatting text to format compatible gensim library","metadata":{}},{"cell_type":"code","source":"words = []\nfor i in df.headline.values:\n    words.append(i.split())\nwords[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:11.689293Z","iopub.execute_input":"2023-09-20T14:52:11.689702Z","iopub.status.idle":"2023-09-20T14:52:11.730059Z","shell.execute_reply.started":"2023-09-20T14:52:11.689668Z","shell.execute_reply":"2023-09-20T14:52:11.728776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deciding on Dimensions of Embedding Vectors\n\nThe dimensions of embedding vectors in NLP tasks are a **hyperparameter** that you need to decide before training your NN. The choice of embedding dimensions can have an impact on the performance of your NLP model, and there's no one-size-fits-all answer. Here are some factors and guidelines to consider when deciding the dimensions of embedding vectors:\n\n1. **Size of Vocabulary**: The size of your vocabulary, which is the total number of unique words in your dataset, can influence the choice of embedding dimensions. If you have a relatively small vocabulary, you might choose smaller embedding dimensions (e.g., 50, 100). For larger vocabularies, you may opt for larger dimensions (e.g., 200, 300).\n\n2. **Amount of Training Data**: The amount of training data you have also plays a role. With more training data, you might be able to use larger embedding dimensions because the model has more examples to learn meaningful representations. Conversely, with limited data, you may want to keep the dimensions smaller to prevent overfitting.\n\n3. **Task Specificity**: The specific NLP task you are working on can influence the choice of embedding dimensions. For example:\n    For tasks like sentiment analysis or text classification, embeddings in the range of 100-300 dimensions are common.\n    For more complex tasks like machine translation or language modeling, larger embeddings (300+ dimensions) may be beneficial.\n\n4. **Pretrained Embeddings**: If you plan to use pretrained word embeddings like Word2Vec, GloVe, or pre-trained embeddings from models like BERT, you should use the same dimensions as the pretrained embeddings to facilitate transfer learning.\n\n5. **Model Architecture**: The choice of model architecture can also affect the embedding dimensions. Some architectures work better with certain embedding dimensions. For example, convolutional neural networks (CNNs) for text classification might benefit from smaller embeddings, while recurrent neural networks (RNNs) or transformers can handle larger embeddings effectively.\n\n6. **Computational Resources**: Consider the computational resources available to you. Larger embedding dimensions require more memory and computational power. Ensure that your hardware can handle the chosen dimensions.\n\n7. **Experimentation**: It's often a good practice to experiment with different embedding dimensions and evaluate their impact on your specific NLP task using validation data. You can perform hyperparameter tuning to find the best embedding dimension for your model's performance.\n\n8. **Visualization**: In some cases, you may want to reduce the dimensions of embeddings (e.g., using techniques like t-SNE or PCA) for visualization and exploration, even if you use larger embeddings for model training.\n\nIn summary, there is no fixed rule for choosing the dimensions of embedding vectors, and it often depends on your specific NLP task, data, and available resources. It's a hyperparameter that should be tuned and experimented with to find the best configuration for your particular use case.","metadata":{}},{"cell_type":"markdown","source":"The Dimension of the vectors we are attempting to generate","metadata":{}},{"cell_type":"code","source":"import gensim\nEMBEDDING_DIM = 200","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:11.732842Z","iopub.execute_input":"2023-09-20T14:52:11.733651Z","iopub.status.idle":"2023-09-20T14:52:33.681849Z","shell.execute_reply.started":"2023-09-20T14:52:11.733602Z","shell.execute_reply":"2023-09-20T14:52:33.680659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating Word Vectors using Word2Vec","metadata":{}},{"cell_type":"code","source":"%time\nw2v_model = gensim.models.Word2Vec(sentences = words , vector_size=EMBEDDING_DIM , window = 5 , min_count = 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:33.683462Z","iopub.execute_input":"2023-09-20T14:52:33.683960Z","iopub.status.idle":"2023-09-20T14:52:36.125525Z","shell.execute_reply.started":"2023-09-20T14:52:33.683912Z","shell.execute_reply":"2023-09-20T14:52:36.124491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Size of vocabulary","metadata":{}},{"cell_type":"code","source":"len(w2v_model.wv.index_to_key)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.126513Z","iopub.execute_input":"2023-09-20T14:52:36.126836Z","iopub.status.idle":"2023-09-20T14:52:36.132698Z","shell.execute_reply.started":"2023-09-20T14:52:36.126806Z","shell.execute_reply":"2023-09-20T14:52:36.131940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"represented each of 38071 words by a 100dim vector.","metadata":{}},{"cell_type":"code","source":"from keras.utils import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.134048Z","iopub.execute_input":"2023-09-20T14:52:36.134624Z","iopub.status.idle":"2023-09-20T14:52:36.147606Z","shell.execute_reply.started":"2023-09-20T14:52:36.134592Z","shell.execute_reply":"2023-09-20T14:52:36.146392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step by Step -\n\n1. `tokenizer = text.Tokenizer(num_words=35000)`: This line initializes a Tokenizer object with a maximum vocabulary size of 35,000 words. The num_words parameter specifies the maximum number of words to keep in the vocabulary, based on word frequency.\n\n2. `tokenizer.fit_on_texts(words)`: This line fits the tokenizer on a list or array of text data called words. During this process, the Tokenizer object learns the vocabulary and assigns a unique integer index to each word in the text data. This is typically a preprocessing step before training a machine learning model.\n\n3. `tokenized_train = tokenizer.texts_to_sequences(words)`: This line converts the text data in the words list into sequences of integers. Each word in the input text is replaced with its corresponding integer index from the tokenizer's vocabulary. The result is stored in the tokenized_train variable.\n\n4. `x = pad_sequences(tokenized_train, maxlen=20)`: This line takes the tokenized sequences in tokenized_train and pads or truncates them to ensure that they all have a fixed length of 20. Padding is added to sequences that are shorter than 20 words, and sequences longer than 20 words are truncated. The resulting x variable contains the padded/truncated sequences\n","metadata":{}},{"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=35000)\ntokenizer.fit_on_texts(words)\ntokenized_train = tokenizer.texts_to_sequences(words)\nx = pad_sequences(tokenized_train, maxlen = 20)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.149313Z","iopub.execute_input":"2023-09-20T14:52:36.149784Z","iopub.status.idle":"2023-09-20T14:52:36.661864Z","shell.execute_reply.started":"2023-09-20T14:52:36.149657Z","shell.execute_reply":"2023-09-20T14:52:36.660391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding 1 because of reserved 0 index\n\nEmbedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n\nThus our vocab size inceeases by 1","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.663680Z","iopub.execute_input":"2023-09-20T14:52:36.664296Z","iopub.status.idle":"2023-09-20T14:52:36.669926Z","shell.execute_reply.started":"2023-09-20T14:52:36.664250Z","shell.execute_reply":"2023-09-20T14:52:36.668642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a function to create weight matrix from word2vec gensim model","metadata":{}},{"cell_type":"code","source":"def get_weight_matrix(model, vocab):\n    # total vocabulary size plus 0 for unknown words\n    vocab_size = len(vocab) + 1\n    # defining the weight matrix dimensions with all 0\n    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    # step vocab, store vectors using the Tokenizer's integer mapping\n    for word, i in vocab.items():\n        weight_matrix[i] = model.wv[word]\n    return weight_matrix","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.671388Z","iopub.execute_input":"2023-09-20T14:52:36.673184Z","iopub.status.idle":"2023-09-20T14:52:36.686769Z","shell.execute_reply.started":"2023-09-20T14:52:36.673142Z","shell.execute_reply":"2023-09-20T14:52:36.685467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer","metadata":{}},{"cell_type":"code","source":"embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.688378Z","iopub.execute_input":"2023-09-20T14:52:36.688758Z","iopub.status.idle":"2023-09-20T14:52:36.839134Z","shell.execute_reply.started":"2023-09-20T14:52:36.688715Z","shell.execute_reply":"2023-09-20T14:52:36.838191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Training the Word2Vec Model","metadata":{}},{"cell_type":"markdown","source":"**Model Creation**","metadata":{}},{"cell_type":"code","source":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=20, trainable=True))\n#LSTM \nmodel.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.3 , dropout = 0.3,return_sequences = True)))\nmodel.add(Bidirectional(GRU(units=32 , recurrent_dropout = 0.1 , dropout = 0.1)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])\n\ndel embedding_vectors","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:36.840710Z","iopub.execute_input":"2023-09-20T14:52:36.841036Z","iopub.status.idle":"2023-09-20T14:52:37.774292Z","shell.execute_reply.started":"2023-09-20T14:52:36.841008Z","shell.execute_reply":"2023-09-20T14:52:37.773413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:37.775752Z","iopub.execute_input":"2023-09-20T14:52:37.776369Z","iopub.status.idle":"2023-09-20T14:52:37.802661Z","shell.execute_reply.started":"2023-09-20T14:52:37.776334Z","shell.execute_reply":"2023-09-20T14:52:37.801553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting Data**","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, df.is_sarcastic , test_size = 0.3 , random_state = 0) ","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:37.803842Z","iopub.execute_input":"2023-09-20T14:52:37.804229Z","iopub.status.idle":"2023-09-20T14:52:37.815570Z","shell.execute_reply.started":"2023-09-20T14:52:37.804198Z","shell.execute_reply":"2023-09-20T14:52:37.814504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Fitting**","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size = 128 , validation_data = (x_test,y_test) , epochs = 10)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T14:52:37.817689Z","iopub.execute_input":"2023-09-20T14:52:37.818060Z","iopub.status.idle":"2023-09-20T15:02:11.064162Z","shell.execute_reply.started":"2023-09-20T14:52:37.818027Z","shell.execute_reply":"2023-09-20T15:02:11.062939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Printing Model Performance**","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100)\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T15:02:11.066089Z","iopub.execute_input":"2023-09-20T15:02:11.066523Z","iopub.status.idle":"2023-09-20T15:02:25.103049Z","shell.execute_reply.started":"2023-09-20T15:02:11.066485Z","shell.execute_reply":"2023-09-20T15:02:25.102062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting Model Performance**","metadata":{}},{"cell_type":"code","source":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T15:02:25.104398Z","iopub.execute_input":"2023-09-20T15:02:25.104716Z","iopub.status.idle":"2023-09-20T15:02:25.687564Z","shell.execute_reply.started":"2023-09-20T15:02:25.104687Z","shell.execute_reply":"2023-09-20T15:02:25.686474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference -\n\n**Looks like our Model is Overfitting**","metadata":{}},{"cell_type":"code","source":"pred = model.predict(x_test) \npred =np.argmax(pred,axis=1)\n\npred[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T15:02:25.688928Z","iopub.execute_input":"2023-09-20T15:02:25.689257Z","iopub.status.idle":"2023-09-20T15:02:31.898625Z","shell.execute_reply.started":"2023-09-20T15:02:25.689226Z","shell.execute_reply":"2023-09-20T15:02:31.897786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test,pred)\ncm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])","metadata":{"execution":{"iopub.status.busy":"2023-09-20T15:02:31.900371Z","iopub.execute_input":"2023-09-20T15:02:31.901120Z","iopub.status.idle":"2023-09-20T15:02:32.179313Z","shell.execute_reply.started":"2023-09-20T15:02:31.901076Z","shell.execute_reply":"2023-09-20T15:02:32.178022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"Use one of the following datasets for Performing - \n\nhttps://www.kaggle.com/datasets/nitin194/twitter-sentiment-analysis?select=train_E6oV3lV.csv\n\nor\n\nhttps://www.kaggle.com/datasets/sunnysai12345/news-summary","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"#### Read and implement GloVe and FastText","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Research and Implement : FastText developed by Facebook","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}}]}